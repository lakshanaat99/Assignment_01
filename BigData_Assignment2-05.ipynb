{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshanaat99/Assignment_01/blob/main/BigData_Assignment2-05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2 - Group 8"
      ],
      "metadata": {
        "id": "qyYyCkZr6XCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1: Environment Setup & Data Ingestion"
      ],
      "metadata": {
        "id": "D3WnDQOw8RA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Install & Configure PySpark"
      ],
      "metadata": {
        "id": "pL8iKIBC6g5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.1\n",
        "!pip install pymongo\n",
        "!pip install dnspython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bygVQQ46mFf",
        "outputId": "81a32a3d-e5fb-4c48-8466-e69abd954f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from pymongo) (2.8.0)\n",
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.12/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ECommerce-BigData-Pipeline\")\n",
        "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.1\")\n",
        "    .getOrCreate())"
      ],
      "metadata": {
        "id": "tzidA12p-nU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MongoDB config\n",
        "# 1. Setup\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# 2. Get Secrets\n",
        "try:\n",
        "    mongo_uri = userdata.get('MONGO_URI')\n",
        "    mongo_db = userdata.get('MONGO_DATABASE') # Fallback if secret missing\n",
        "\n",
        "    print(f\"Attempting to connect with User: {mongo_uri.split(':')[1].split('@')[0]}...\") # Prints username only for debug\n",
        "except Exception as e:\n",
        "    print(\"Error reading secrets! Make sure 'Notebook access' is ON in the sidebar.\")\n",
        "    raise e\n",
        "\n",
        "# 3. Connect & Test\n",
        "os.environ[\"MONGO_URI\"] = mongo_uri\n",
        "os.environ[\"MONGO_DATABASE\"] = mongo_db\n",
        "\n",
        "# 4. Verify connection\n",
        "try:\n",
        "    # Using the environment variable to connect, proving it works\n",
        "    client = MongoClient(os.environ[\"MONGO_URI\"])\n",
        "    client.admin.command('ping')\n",
        "\n",
        "    print(\"Authentication Successful! Connected to MongoDB Atlas.\")\n",
        "    print(f\"Environment variables set for database: {os.environ['MONGO_DATABASE']}\")\n",
        "    print(\"Ready for Spark integration.\")\n",
        "except Exception as e:\n",
        "    print(f\"Connection failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYBTOQCe2YdB",
        "outputId": "5c067a37-113f-4a4e-dd3c-739a52ce11ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to connect with User: //Tharindu06...\n",
            "Authentication Successful! Connected to MongoDB Atlas.\n",
            "Environment variables set for database: bigdata_assignment02\n",
            "Ready for Spark integration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load Dataset"
      ],
      "metadata": {
        "id": "Ccc9LUCj_jEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8DZhrFOkDYD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374cf9ef-a3ef-415e-9c17-8d089f6aee1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession class from PySpark SQL module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Read the CSV file into a Spark DataFrame\n",
        "raw_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/drive/MyDrive/Bigdata Assignment 2/Online retail.csv\")"
      ],
      "metadata": {
        "id": "Vd43EHE__nsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Display schema and record counts"
      ],
      "metadata": {
        "id": "SECTXWGVGmtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.printSchema()\n",
        "raw_df.count()"
      ],
      "metadata": {
        "id": "Ulk-hf0kGqeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24f44d5-3de8-4341-dc18-849401b611ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Invoice: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Customer ID: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "541910"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Store Bronze data as partitioned Parquet"
      ],
      "metadata": {
        "id": "WezmIFQwHNPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month, col, to_timestamp\n",
        "\n",
        "bronze_df = raw_df.withColumn(\n",
        "    \"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"M/d/yyyy H:mm\")\n",
        ").withColumn(\n",
        "    \"year\", year(\"InvoiceDate\")\n",
        ").withColumn(\n",
        "    \"month\", month(\"InvoiceDate\")\n",
        ")"
      ],
      "metadata": {
        "id": "13g6ij8uHSHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"year\", \"month\") \\\n",
        "    .parquet(\"/content/bronze\")"
      ],
      "metadata": {
        "id": "1NEi9xu1Hb8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Data Cleaning & Quality Management"
      ],
      "metadata": {
        "id": "1OFrUcR3ID7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Missing CustomerID Handling"
      ],
      "metadata": {
        "id": "XD9J3jGIJKH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when\n",
        "\n",
        "null_customer = bronze_df.filter(col(\"Customer ID\").isNull()).count()\n",
        "print(f\"Null CustomerID records: {null_customer:,}\")"
      ],
      "metadata": {
        "id": "cL_KeFbjJOnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fbda212-0b49-4286-c516-15c8eae16f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null CustomerID records: 135,080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove null\n",
        "df1 = bronze_df.filter(col(\"Customer ID\").isNotNull())\n",
        "print(f\"After removing null CustomerID: {df1.count():,}\")"
      ],
      "metadata": {
        "id": "7p-rlqMeZowp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5ae581-513f-4b1f-a1db-1786cdd75dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing null CustomerID: 406,830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Negative quantities (returns)"
      ],
      "metadata": {
        "id": "ysnE7OLJJt3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_qty_count = df1.filter(col(\"Quantity\") < 0).count()\n",
        "print(f\"Negative Quantity records: {negative_qty_count:,}\")\n",
        "df2 = df1.filter(col(\"Quantity\") > 0)\n",
        "print(f\"After removing returns: {df2.count():,}\")"
      ],
      "metadata": {
        "id": "j--ZGiTuKJdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d723e776-4825-483b-ae96-579d2923e24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative Quantity records: 8,905\n",
            "After removing returns: 397,925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Cancelled invoices"
      ],
      "metadata": {
        "id": "Wg5cTaRvKwjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancelled_df = df2.filter(col(\"Invoice\").startswith(\"C\")).count()\n",
        "print(f\"Cancelled invoices (C prefix): {cancelled_df:,}\")\n",
        "df3 = df2.filter(~col(\"Invoice\").startswith(\"C\"))\n",
        "print(f\"After removing cancellations: {df3.count():,}\")"
      ],
      "metadata": {
        "id": "tgeo0w2By-CU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077e5cf9-f6aa-4179-aeba-ca90d124a602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancelled invoices (C prefix): 0\n",
            "After removing cancellations: 397,925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Invalid or extreme prices"
      ],
      "metadata": {
        "id": "8hVFO9aqLHKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_price_df = df3.filter(col(\"Price\") <= 0)\n",
        "invalid_price_count = invalid_price_df.count()\n",
        "print(f\"Invalid prices (<=0): {invalid_price_count:,}\")\n",
        "df4 = df3.filter(col(\"Price\") > 0)\n",
        "print(f\"After price filter: {df4.count():,}\")"
      ],
      "metadata": {
        "id": "HyLNB_ewLaq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70934eec-4c7c-4aaa-c43f-ab8d507af635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid prices (<=0): 40\n",
            "After price filter: 397,885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Duplicate records"
      ],
      "metadata": {
        "id": "UdPiwETILHuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = df4.count()\n",
        "silver_df = df4.dropDuplicates()\n",
        "after = silver_df.count()\n",
        "duplicates_removed = before - after\n",
        "print(f\"Duplicates removed: {duplicates_removed:,}\")\n",
        "print(f\"Final Silver records: {after:,}\")"
      ],
      "metadata": {
        "id": "VuUxhMJgL7Sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656dd668-6dbf-4040-d23b-838b8644fb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates removed: 5,192\n",
            "Final Silver records: 392,693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Data Quality Report"
      ],
      "metadata": {
        "id": "ABNWIDZELINq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dq_report_df = spark.createDataFrame(\n",
        "    [{ \"Metric\": \"Total Raw Records\", \"Count\": raw_df.count() },\n",
        "    { \"Metric\": \"Null CustomerID Removed\", \"Count\": null_customer },\n",
        "    { \"Metric\": \"Negative Quantity Removed\", \"Count\": negative_qty_count },\n",
        "    { \"Metric\": \"Cancelled Invoices Removed\", \"Count\": cancelled_df },\n",
        "    { \"Metric\": \"Invalid Prices Removed\", \"Count\": invalid_price_count },\n",
        "    { \"Metric\": \"Duplicates Removed\", \"Count\": duplicates_removed },\n",
        "    { \"Metric\": \"FINAL SILVER Records\", \"Count\": after }],\n",
        "    [\"Metric\", \"Count\"])\n",
        "\n",
        "print(\"\\n DATA QUALITY REPORT\")\n",
        "dq_report_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "2kr15JSoNMdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdcb9363-3e2c-4ee0-c022-7bc9bd8e054c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " DATA QUALITY REPORT\n",
            "+------+--------------------------+\n",
            "|Metric|Count                     |\n",
            "+------+--------------------------+\n",
            "|541910|Total Raw Records         |\n",
            "|135080|Null CustomerID Removed   |\n",
            "|8905  |Negative Quantity Removed |\n",
            "|0     |Cancelled Invoices Removed|\n",
            "|40    |Invalid Prices Removed    |\n",
            "|5192  |Duplicates Removed        |\n",
            "|392693|FINAL SILVER Records      |\n",
            "+------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to Drive as CSV\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "try:\n",
        "    dq_report_df.toPandas().to_csv('data_quality_report.csv', index=False)\n",
        "    print(\"\\n Data Quality Report saved as 'data_quality_report.csv'\")\n",
        "except:\n",
        "    print(\"\\n Could not save to Drive - check mounting\")\n"
      ],
      "metadata": {
        "id": "8vWz_wBuafE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d1f9615-cf0b-4e09-9d32-cd072025b815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Data Quality Report saved as 'data_quality_report.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Feature Engineering"
      ],
      "metadata": {
        "id": "Hh0uGk1jXxuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Load Silver Layer"
      ],
      "metadata": {
        "id": "-xj1102pXye2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the silver dataframe to the path so it can be loaded later\n",
        "silver_df.write.mode(\"overwrite\").parquet(\"/content/silver/online_retail\")\n",
        "\n",
        "# Read the silver layer from the parquet file\n",
        "df_silver = spark.read.parquet(\"/content/silver/online_retail\")\n",
        "df_silver.printSchema()"
      ],
      "metadata": {
        "id": "s_sxUK2zYF-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3205f276-5a2d-4716-bbfd-4d293ae49ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Invoice: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: timestamp (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Customer ID: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Revenue calculation"
      ],
      "metadata": {
        "id": "ROW_9bkDXzIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the col function to reference DataFrame columns\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a new column 'revenue' by multiplying Quantity and UnitPrice\n",
        "df_feat = df_silver.withColumn(\n",
        "    \"revenue\",\n",
        "    col(\"Quantity\") * col(\"Price\")\n",
        ")"
      ],
      "metadata": {
        "id": "p8xdxKRBcjt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Time-based features (hour, weekday, month)"
      ],
      "metadata": {
        "id": "lurLFRhzXztd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import time-related functions from PySpark\n",
        "from pyspark.sql.functions import hour, dayofweek, month\n",
        "\n",
        "# Extract hour, day of week, and month from the InvoiceDate column\n",
        "df_feat = df_feat.withColumn(\"invoice_hour\", hour(\"InvoiceDate\")) \\\n",
        "                 .withColumn(\"weekday\", dayofweek(\"InvoiceDate\")) \\\n",
        "                 .withColumn(\"invoice_month\", month(\"InvoiceDate\"))"
      ],
      "metadata": {
        "id": "8BGbvM1Bcthj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Basket-level metrics"
      ],
      "metadata": {
        "id": "RCvokheeX0P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import aggregation functions from PySpark\n",
        "from pyspark.sql.functions import sum, countDistinct\n",
        "\n",
        "# Group by Invoice to calculate total revenue and number of unique items per basket\n",
        "basket_df = df_feat.groupBy(\"Invoice\").agg(\n",
        "    sum(\"revenue\").alias(\"basket_revenue\"),\n",
        "    countDistinct(\"StockCode\").alias(\"items_per_basket\")\n",
        ")\n",
        "\n",
        "# Join the basket metrics back to the main features dataframe\n",
        "df_feat = df_feat.join(basket_df, on=\"Invoice\", how=\"left\")"
      ],
      "metadata": {
        "id": "hyRGDcBXc3jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Customer RFM features"
      ],
      "metadata": {
        "id": "cccEFlpMX0zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import functions for calculating dates and maximum values\n",
        "from pyspark.sql.functions import max, datediff, current_date, countDistinct, sum\n",
        "\n",
        "# Group by Customer ID to calculate RFM (Recency, Frequency, Monetary) metrics:\n",
        "# Recency: Days between today and the last purchase\n",
        "# Frequency: Count of unique invoice numbers\n",
        "# Monetary: Sum of total revenue\n",
        "rfm_df = df_feat.groupBy(\"Customer ID\").agg(\n",
        "    datediff(current_date(), max(\"InvoiceDate\")).alias(\"recency\"),\n",
        "    countDistinct(\"Invoice\").alias(\"frequency\"),\n",
        "    sum(\"revenue\").alias(\"monetary\")\n",
        ")\n",
        "\n",
        "# Join the customer-level RFM features back to the main dataframe\n",
        "df_feat = df_feat.join(rfm_df, on=\"Customer ID\", how=\"left\")"
      ],
      "metadata": {
        "id": "lw_7WQXZc_U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Window-Based Feature"
      ],
      "metadata": {
        "id": "o2WCSf8fX1TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Window functionality and the sum aggregation function\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# Define a window partitioned by Customer ID and ordered by InvoiceDate\n",
        "# It covers all rows from the start of the customer's history up to the current row\n",
        "customer_window = Window.partitionBy(\"Customer ID\") \\\n",
        "                        .orderBy(\"InvoiceDate\") \\\n",
        "                        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Calculate a running total of revenue for each customer using the defined window\n",
        "df_feat = df_feat.withColumn(\n",
        "    \"running_customer_spend\",\n",
        "    sum(\"revenue\").over(customer_window)\n",
        ")"
      ],
      "metadata": {
        "id": "0rSV5BR9dI_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Save Feature-Engineered Data"
      ],
      "metadata": {
        "id": "74NcRNOTX1yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the feature-engineered DataFrame to a Parquet file for persistence\n",
        "df_feat.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"/content/feature_engineered/online_retail\")"
      ],
      "metadata": {
        "id": "zErans3ydPs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: MongoDB Data Modeling"
      ],
      "metadata": {
        "id": "ybt3odJXP9b6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Start Spark with MongoDB Connector"
      ],
      "metadata": {
        "id": "ULRZvqu_i2CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# Create Spark session and tell Spark how to connect to MongoDB\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigData_Task4_MongoDB_GoldLayer\") \\\n",
        "    .config(\n",
        "        \"spark.mongodb.write.connection.uri\",\n",
        "        os.environ[\"MONGO_URI\"]\n",
        "    ) \\\n",
        "    .config(\n",
        "        \"spark.mongodb.write.database\",\n",
        "        os.environ[\"MONGO_DATABASE\"]\n",
        "    ) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session started successfully with MongoDB connector.\")"
      ],
      "metadata": {
        "id": "Kr5Vz27Ci-EF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c31273-74b7-4486-99cb-f63afa673a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session started successfully with MongoDB connector.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Load Feature-Engineered Data"
      ],
      "metadata": {
        "id": "F9nfTULjQGcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the feature-engineered dataset created in Task 3\n",
        "df_feat = spark.read.parquet(\"/content/feature_engineered/online_retail\")"
      ],
      "metadata": {
        "id": "8eW3Gx1mRKNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 COLLECTION 1 - fact_invoices"
      ],
      "metadata": {
        "id": "K1hP0A3GQKHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create invoice-level structure"
      ],
      "metadata": {
        "id": "2dLU3NlNWagM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import first, sum, collect_list, struct\n",
        "\n",
        "# Group by Invoice to create invoice-level documents\n",
        "fact_invoices_df = df_feat.groupBy(\"Invoice\").agg(\n",
        "\n",
        "    # Customer who placed the invoice\n",
        "    first(\"Customer ID\").alias(\"CustomerID\"),\n",
        "\n",
        "    # Invoice date\n",
        "    first(\"InvoiceDate\").alias(\"InvoiceDate\"),\n",
        "\n",
        "    # Country of purchase\n",
        "    first(\"Country\").alias(\"Country\"),\n",
        "\n",
        "    # Total revenue of the invoice\n",
        "    sum(\"revenue\").alias(\"total_invoice_revenue\"),\n",
        "\n",
        "    # Embed purchased items as an array\n",
        "    collect_list(\n",
        "        struct(\n",
        "            \"StockCode\",\n",
        "            \"Description\",\n",
        "            \"Quantity\",\n",
        "            \"Price\",\n",
        "            \"revenue\"\n",
        "        )\n",
        "    ).alias(\"line_items\")\n",
        ")"
      ],
      "metadata": {
        "id": "LXv0U2HsWgSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Schema Definition (fact_invoices)**\n",
        "\n",
        "{\n",
        "  _id: ObjectId,\n",
        "  Invoice: String,\n",
        "  CustomerID: Integer,\n",
        "  InvoiceDate: Date,\n",
        "  Country: String,\n",
        "  total_invoice_revenue: Double,\n",
        "  line_items: [\n",
        "    {\n",
        "      StockCode: String,\n",
        "      Description: String,\n",
        "      Quantity: Integer,\n",
        "      Price: Double,\n",
        "      revenue: Double\n",
        "    }\n",
        "  ]\n",
        "}"
      ],
      "metadata": {
        "id": "YMMFCPZeqFPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 COLLECTION 2 - dim_customers"
      ],
      "metadata": {
        "id": "1ruvskw9QTXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create customer summary"
      ],
      "metadata": {
        "id": "8_vpaxidX45B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import countDistinct, col, when\n",
        "\n",
        "# Create customer-level aggregation\n",
        "dim_customers_df = df_feat.groupBy(\"Customer ID\").agg(\n",
        "\n",
        "    # Recency (days since last purchase)\n",
        "    first(\"recency\").alias(\"recency\"),\n",
        "\n",
        "    # Frequency (number of invoices)\n",
        "    first(\"frequency\").alias(\"frequency\"),\n",
        "\n",
        "    # Monetary (total spend)\n",
        "    first(\"monetary\").alias(\"monetary\"),\n",
        "\n",
        "    # Total invoices count\n",
        "    countDistinct(\"Invoice\").alias(\"total_invoices\"),\n",
        "\n",
        "    # Total revenue from customer\n",
        "    sum(\"revenue\").alias(\"total_revenue\")\n",
        ")"
      ],
      "metadata": {
        "id": "R56oVl49Xlaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add customer segmentation"
      ],
      "metadata": {
        "id": "KT7nDmN7YCJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Segment customers based on monetary value\n",
        "dim_customers_df = dim_customers_df.withColumn(\n",
        "    \"customer_segment\",\n",
        "    when(col(\"monetary\") >= 5000, \"High Value\")\n",
        "    .when(col(\"monetary\") >= 2000, \"Medium Value\")\n",
        "    .otherwise(\"Low Value\")\n",
        ")"
      ],
      "metadata": {
        "id": "9m21ZgR3YNZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 COLLECTION 3 - dim_products"
      ],
      "metadata": {
        "id": "UI3-lfvuQVog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create product summary"
      ],
      "metadata": {
        "id": "gcvXMwTHZldv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import collect_set\n",
        "\n",
        "# Create product-level performance data\n",
        "dim_products_df = df_feat.groupBy(\"StockCode\", \"Description\").agg(\n",
        "\n",
        "    # Total quantity sold\n",
        "    sum(\"Quantity\").alias(\"total_quantity_sold\"),\n",
        "\n",
        "    # Total revenue of product\n",
        "    sum(\"revenue\").alias(\"total_product_revenue\"),\n",
        "\n",
        "    # Countries where product was sold\n",
        "    collect_set(\"Country\").alias(\"countries_sold\")\n",
        ")"
      ],
      "metadata": {
        "id": "H-kJKfe0Y3VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5 MongoDB Indexing & Write Optimization"
      ],
      "metadata": {
        "id": "NYZ1rT6hBVIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 Write Gold datasets to MongoDB"
      ],
      "metadata": {
        "id": "spPTYWZYCw1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write to MongoDB (V10 Syntax)\n",
        "fact_invoices_df.write \\\n",
        "    .format(\"mongodb\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"spark.mongodb.connection.uri\", os.environ.get(\"MONGO_URI\")) \\\n",
        "    .option(\"spark.mongodb.database\", os.environ.get(\"MONGO_DATABASE\")) \\\n",
        "    .option(\"spark.mongodb.collection\", \"fact_invoices\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"Data successfully written to MongoDB!\")"
      ],
      "metadata": {
        "id": "wGIRcS7guT_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543cf3ae-dd67-49bc-830d-36ab7bb90550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully written to MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write dim_customers to MongoDB (V10 Syntax)\n",
        "dim_customers_df.write \\\n",
        "    .format(\"mongodb\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"spark.mongodb.connection.uri\", os.environ.get(\"MONGO_URI\")) \\\n",
        "    .option(\"spark.mongodb.database\", os.environ.get(\"MONGO_DATABASE\")) \\\n",
        "    .option(\"spark.mongodb.collection\", \"dim_customers\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"dim_customers data successfully written to MongoDB!\")"
      ],
      "metadata": {
        "id": "wGg20d2kzCq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f133706-210e-457c-f215-670ea085b66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim_customers data successfully written to MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write dim_products to MongoDB (V10 Syntax)\n",
        "dim_products_df.write \\\n",
        "    .format(\"mongodb\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"spark.mongodb.connection.uri\", os.environ.get(\"MONGO_URI\")) \\\n",
        "    .option(\"spark.mongodb.database\", os.environ.get(\"MONGO_DATABASE\")) \\\n",
        "    .option(\"spark.mongodb.collection\", \"dim_products\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"dim_products data successfully written to MongoDB!\")"
      ],
      "metadata": {
        "id": "G_JItyCGzSAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d0e58b4-17c0-4158-e737-93ec5d6a603f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim_products data successfully written to MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 Create minimum 4 indexes"
      ],
      "metadata": {
        "id": "ebKXRPPNCE9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyMongo tools for indexing\n",
        "from pymongo import MongoClient, ASCENDING, DESCENDING\n",
        "import os\n",
        "\n",
        "# Connect to MongoDB using existing environment variables\n",
        "client = MongoClient(os.environ[\"MONGO_URI\"])\n",
        "db = client[os.environ[\"MONGO_DATABASE\"]]\n",
        "\n",
        "print(\"Creating Indexes...\")"
      ],
      "metadata": {
        "id": "JHKhKdwuCIpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7eb890b-ed8b-4c7e-d8ee-2725511b15ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Indexes...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Index on CustomerID (Ascending)\n",
        "db.fact_invoices.create_index([(\"CustomerID\", ASCENDING)])\n",
        "print(\"Index 1 Created: fact_invoices.CustomerID (ASC)\")"
      ],
      "metadata": {
        "id": "KstuTD_YDffv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe07999-910b-4c01-a4bd-80dd5729ce34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 1 Created: fact_invoices.CustomerID (ASC)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Index on InvoiceDate (Descending)\n",
        "db.fact_invoices.create_index([(\"InvoiceDate\", DESCENDING)])\n",
        "print(\"Index 2 Created: fact_invoices.InvoiceDate (DESC)\")"
      ],
      "metadata": {
        "id": "NOWo3YmSDiQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e6c400-9917-4263-df76-b6ac56ef1476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 2 Created: fact_invoices.InvoiceDate (DESC)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Index on customer_segment (Ascending)\n",
        "db.dim_customers.create_index([(\"customer_segment\", ASCENDING)])\n",
        "print(\"Index 3 Created: dim_customers.customer_segment (ASC)\")"
      ],
      "metadata": {
        "id": "Tbo6KW_-DkJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f759ea-630d-436e-9ba5-12d592daea7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 3 Created: dim_customers.customer_segment (ASC)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Index on total_product_revenue (Descending)\n",
        "db.dim_products.create_index([(\"total_product_revenue\", DESCENDING)])\n",
        "print(\"Index 4 Created: dim_products.total_product_revenue (DESC)\")"
      ],
      "metadata": {
        "id": "PQ4HT5BzDm-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23d768a-1520-4adc-fbb3-3c37227444e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 4 Created: dim_products.total_product_revenue (DESC)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3 Justify each index"
      ],
      "metadata": {
        "id": "0i8Ve9ZmJEr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On report"
      ],
      "metadata": {
        "id": "0QDvhGjsJVNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.4 Demonstrate query performance improvement"
      ],
      "metadata": {
        "id": "z-6xTUIgJObZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print(\"Testing Performance on CustomerID Query...\")\n",
        "\n",
        "# sample query (finding invoices for customer 17850)\n",
        "sample_query = {\"CustomerID\": 17850}\n",
        "\n",
        "#MongoDB explain how it will execute this query\n",
        "explanation = db.fact_invoices.find(sample_query).explain()\n",
        "\n",
        "# Extract the winning execution plan strategy\n",
        "winning_stage = explanation['queryPlanner']['winningPlan']['stage']\n",
        "\n",
        "# Sometimes the IXSCAN is nested under a FETCH stage\n",
        "if winning_stage == 'FETCH':\n",
        "    winning_stage = explanation['queryPlanner']['winningPlan']['inputStage']['stage']\n",
        "\n",
        "print(f\"Execution Strategy Used: {winning_stage}\")\n",
        "\n",
        "if winning_stage == \"IXSCAN\":\n",
        "    print(\"SUCCESS! MongoDB utilized the Index (IXSCAN).\")\n",
        "    print(\"Without the index, this would have been a 'COLLSCAN', forcing MongoDB to read every single document.\")\n",
        "else:\n",
        "    print(\"Warning: Index was not used.\")"
      ],
      "metadata": {
        "id": "ngHbKS9YJLQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ed0e47-a3d8-491a-b352-b7293eaf8002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Performance on CustomerID Query...\n",
            "Execution Strategy Used: IXSCAN\n",
            "SUCCESS! MongoDB utilized the Index (IXSCAN).\n",
            "Without the index, this would have been a 'COLLSCAN', forcing MongoDB to read every single document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 6 (Analytics & Insights)"
      ],
      "metadata": {
        "id": "pP2xZD06K2vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Spark-based analytics queries"
      ],
      "metadata": {
        "id": "EGJmURD_K8xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, desc, count, col\n",
        "\n",
        "# 1. Monthly Revenue Trends\n",
        "print(\"1. Monthly Revenue Trends\")\n",
        "monthly_revenue_spark = df_feat.groupBy(\"year\", \"month\") \\\n",
        "    .agg(sum(\"revenue\").alias(\"Total_Revenue\")) \\\n",
        "    .orderBy(\"year\", \"month\")\n",
        "monthly_revenue_spark.show()\n",
        "\n",
        "# 2. Top Customers by Spend\n",
        "print(\"2. Top 5 Customers by Spend\")\n",
        "top_customers_spark = dim_customers_df \\\n",
        "    .orderBy(desc(\"monetary\")) \\\n",
        "    .select(\"Customer ID\", \"monetary\", \"total_invoices\", \"customer_segment\")\n",
        "top_customers_spark.show(5)\n",
        "\n",
        "# 3. Top Products by Revenue\n",
        "print(\"3. Top 5 Products by Revenue\")\n",
        "top_products_spark = dim_products_df \\\n",
        "    .orderBy(desc(\"total_product_revenue\")) \\\n",
        "    .select(\"StockCode\", \"Description\", \"total_product_revenue\", \"total_quantity_sold\")\n",
        "top_products_spark.show(5)\n",
        "\n",
        "# 4. Country-Level Sales Analysis\n",
        "print(\"4. Top 5 Countries by Sales Revenue\")\n",
        "country_sales_spark = df_feat.groupBy(\"Country\") \\\n",
        "    .agg(sum(\"revenue\").alias(\"Total_Revenue\"), count(\"Invoice\").alias(\"Total_Transactions\")) \\\n",
        "    .orderBy(desc(\"Total_Revenue\"))\n",
        "country_sales_spark.show(5)\n",
        "\n",
        "# 5. Return or Cancellation Patterns (Using Bronze Data)\n",
        "print(\"5. Most Frequently Returned/Cancelled Products\")\n",
        "# We use bronze_df here because Silver/Gold layers filtered out returns!\n",
        "returns_spark = bronze_df.filter(col(\"Quantity\") < 0) \\\n",
        "    .groupBy(\"Description\") \\\n",
        "    .agg(sum(\"Quantity\").alias(\"Total_Returned_Qty\"), count(\"Invoice\").alias(\"Return_Count\")) \\\n",
        "    .orderBy(\"Total_Returned_Qty\") # Ordering ascending because quantities are negative\n",
        "returns_spark.show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "csuWoJk9K6kd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6261d308-d776-4b44-dded-12acd222f86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Monthly Revenue Trends\n",
            "+----+-----+------------------+\n",
            "|year|month|     Total_Revenue|\n",
            "+----+-----+------------------+\n",
            "|2010|   12| 570422.7300000137|\n",
            "|2011|    1| 568101.3100000113|\n",
            "|2011|    2| 446084.9200000042|\n",
            "|2011|    3| 594081.7600000091|\n",
            "|2011|    4| 468374.3310000021|\n",
            "|2011|    5| 677355.1500000051|\n",
            "|2011|    6| 660046.0500000059|\n",
            "|2011|    7| 598962.9010000031|\n",
            "|2011|    8|  644051.040000007|\n",
            "|2011|    9| 950690.2020000173|\n",
            "|2011|   10|1035642.4500000259|\n",
            "|2011|   11|1156205.6100000308|\n",
            "|2011|   12|517208.44000000134|\n",
            "+----+-----+------------------+\n",
            "\n",
            "2. Top 5 Customers by Spend\n",
            "+-----------+------------------+--------------+----------------+\n",
            "|Customer ID|          monetary|total_invoices|customer_segment|\n",
            "+-----------+------------------+--------------+----------------+\n",
            "|      14646|280206.01999999996|            73|      High Value|\n",
            "|      18102|          259657.3|            60|      High Value|\n",
            "|      17450|194390.78999999995|            46|      High Value|\n",
            "|      16446|          168472.5|             2|      High Value|\n",
            "|      14911|143711.16999999998|           201|      High Value|\n",
            "+-----------+------------------+--------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "3. Top 5 Products by Revenue\n",
            "+---------+--------------------+---------------------+-------------------+\n",
            "|StockCode|         Description|total_product_revenue|total_quantity_sold|\n",
            "+---------+--------------------+---------------------+-------------------+\n",
            "|    23843|PAPER CRAFT , LIT...|             168469.6|              80995|\n",
            "|    22423|REGENCY CAKESTAND...|   142264.74999999977|              12374|\n",
            "|   85123A|WHITE HANGING HEA...|   100392.09999999993|              36706|\n",
            "|   85099B|JUMBO BAG RED RET...|    85040.54000000033|              46078|\n",
            "|    23166|MEDIUM CERAMIC TO...|    81416.72999999998|              77916|\n",
            "+---------+--------------------+---------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "4. Top 5 Countries by Sales Revenue\n",
            "+--------------+------------------+------------------+\n",
            "|       Country|     Total_Revenue|Total_Transactions|\n",
            "+--------------+------------------+------------------+\n",
            "|United Kingdom| 7285024.644000474|            349203|\n",
            "|   Netherlands|285446.33999999985|              2359|\n",
            "|          EIRE| 265262.4599999989|              7226|\n",
            "|       Germany|228678.39999999997|              9025|\n",
            "|        France| 208952.3099999995|              8327|\n",
            "+--------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "5. Most Frequently Returned/Cancelled Products\n",
            "+------------------------------+------------------+------------+\n",
            "|Description                   |Total_Returned_Qty|Return_Count|\n",
            "+------------------------------+------------------+------------+\n",
            "|PAPER CRAFT , LITTLE BIRDIE   |-80995            |1           |\n",
            "|MEDIUM CERAMIC TOP STORAGE JAR|-74494            |10          |\n",
            "|NULL                          |-46156            |862         |\n",
            "|printing smudges/thrown away  |-19200            |2           |\n",
            "|Unsaleable, destroyed.        |-15644            |9           |\n",
            "+------------------------------+------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 MongoDB Aggregation Pipelines"
      ],
      "metadata": {
        "id": "zAfgzRwdLaal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Monthly Revenue Trends (Using fact_invoices)\n",
        "print(\"1. Monthly Revenue Trends (MongoDB)\")\n",
        "pipeline_monthly = [\n",
        "    {\"$group\": {\n",
        "        \"_id\": {\"year\": {\"$year\": \"$InvoiceDate\"}, \"month\": {\"$month\": \"$InvoiceDate\"}},\n",
        "        \"total_revenue\": {\"$sum\": \"$total_invoice_revenue\"}\n",
        "    }},\n",
        "    {\"$sort\": {\"_id.year\": 1, \"_id.month\": 1}}\n",
        "]\n",
        "res_monthly = list(db.fact_invoices.aggregate(pipeline_monthly))\n",
        "# Using pandas just for clean printing in Colab\n",
        "print(pd.json_normalize(res_monthly).to_string(index=False), \"\\n\")\n",
        "\n",
        "\n",
        "# 2. Top Customers by Spend (Using dim_customers)\n",
        "print(\"2. Top 5 Customers by Spend (MongoDB)\")\n",
        "pipeline_top_cust = [\n",
        "    {\"$sort\": {\"monetary\": -1}},\n",
        "    {\"$limit\": 5},\n",
        "    {\"$project\": {\"Customer ID\": 1, \"monetary\": 1, \"customer_segment\": 1, \"_id\": 0}}\n",
        "]\n",
        "res_top_cust = list(db.dim_customers.aggregate(pipeline_top_cust))\n",
        "print(pd.DataFrame(res_top_cust).to_string(index=False), \"\\n\")\n",
        "\n",
        "\n",
        "# 3. Top Products by Revenue (Using dim_products)\n",
        "print(\"3. Top 5 Products by Revenue (MongoDB)\")\n",
        "pipeline_top_prod = [\n",
        "    {\"$sort\": {\"total_product_revenue\": -1}},\n",
        "    {\"$limit\": 5},\n",
        "    {\"$project\": {\"Description\": 1, \"total_product_revenue\": 1, \"total_quantity_sold\": 1, \"_id\": 0}}\n",
        "]\n",
        "res_top_prod = list(db.dim_products.aggregate(pipeline_top_prod))\n",
        "print(pd.DataFrame(res_top_prod).to_string(index=False), \"\\n\")\n",
        "\n",
        "\n",
        "# 4. Country-Level Sales Analysis (Using fact_invoices)\n",
        "print(\"4. Top 5 Countries by Sales (MongoDB)\")\n",
        "pipeline_country = [\n",
        "    {\"$group\": {\n",
        "        \"_id\": \"$Country\",\n",
        "        \"total_sales\": {\"$sum\": \"$total_invoice_revenue\"},\n",
        "        \"invoice_count\": {\"$sum\": 1}\n",
        "    }},\n",
        "    {\"$sort\": {\"total_sales\": -1}},\n",
        "    {\"$limit\": 5}\n",
        "]\n",
        "res_country = list(db.fact_invoices.aggregate(pipeline_country))\n",
        "print(pd.DataFrame(res_country).to_string(index=False), \"\\n\")\n",
        "\n",
        "\n",
        "# 5. Customer Segmentation Distribution (Using dim_customers)\n",
        "print(\"5. Customer Segmentation Value (MongoDB)\")\n",
        "pipeline_segments = [\n",
        "    {\"$group\": {\n",
        "        \"_id\": \"$customer_segment\",\n",
        "        \"customer_count\": {\"$sum\": 1},\n",
        "        \"total_segment_revenue\": {\"$sum\": \"$monetary\"}\n",
        "    }},\n",
        "    {\"$sort\": {\"total_segment_revenue\": -1}}\n",
        "]\n",
        "res_segments = list(db.dim_customers.aggregate(pipeline_segments))\n",
        "print(pd.DataFrame(res_segments).to_string(index=False), \"\\n\")"
      ],
      "metadata": {
        "id": "MebRnML6LSKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dcb952d-e8f8-43cc-cf3e-f7736fe7f331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Monthly Revenue Trends (MongoDB)\n",
            " total_revenue  _id.year  _id.month\n",
            "    570422.730      2010         12\n",
            "    568101.310      2011          1\n",
            "    446084.920      2011          2\n",
            "    594081.760      2011          3\n",
            "    468374.331      2011          4\n",
            "    677355.150      2011          5\n",
            "    660046.050      2011          6\n",
            "    598962.901      2011          7\n",
            "    644051.040      2011          8\n",
            "    950690.202      2011          9\n",
            "   1035642.450      2011         10\n",
            "   1156205.610      2011         11\n",
            "    517208.440      2011         12 \n",
            "\n",
            "2. Top 5 Customers by Spend (MongoDB)\n",
            " Customer ID  monetary customer_segment\n",
            "       14646 280206.02       High Value\n",
            "       18102 259657.30       High Value\n",
            "       17450 194390.79       High Value\n",
            "       16446 168472.50       High Value\n",
            "       14911 143711.17       High Value \n",
            "\n",
            "3. Top 5 Products by Revenue (MongoDB)\n",
            "                       Description  total_quantity_sold  total_product_revenue\n",
            "       PAPER CRAFT , LITTLE BIRDIE                80995              168469.60\n",
            "          REGENCY CAKESTAND 3 TIER                12374              142264.75\n",
            "WHITE HANGING HEART T-LIGHT HOLDER                36706              100392.10\n",
            "           JUMBO BAG RED RETROSPOT                46078               85040.54\n",
            "    MEDIUM CERAMIC TOP STORAGE JAR                77916               81416.73 \n",
            "\n",
            "4. Top 5 Countries by Sales (MongoDB)\n",
            "           _id  total_sales  invoice_count\n",
            "United Kingdom  7285024.644          16646\n",
            "   Netherlands   285446.340             94\n",
            "          EIRE   265262.460            260\n",
            "       Germany   228678.400            457\n",
            "        France   208952.310            389 \n",
            "\n",
            "5. Customer Segmentation Value (MongoDB)\n",
            "         _id  customer_count  total_segment_revenue\n",
            "  High Value             274            4790654.330\n",
            "   Low Value            3442            2193179.753\n",
            "Medium Value             622            1903392.811 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7 Performance Optimization"
      ],
      "metadata": {
        "id": "G4GEClJ1MWDq"
      }
    }
  ]
}