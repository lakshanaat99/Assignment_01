{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshanaat99/Assignment_01/blob/main/BigData_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2 - Group 8"
      ],
      "metadata": {
        "id": "qyYyCkZr6XCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 1: Environment Setup & Data Ingestion"
      ],
      "metadata": {
        "id": "D3WnDQOw8RA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Install & Configure PySpark"
      ],
      "metadata": {
        "id": "pL8iKIBC6g5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.5.1\n",
        "!pip install pymongo\n",
        "!pip install dnspython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bygVQQ46mFf",
        "outputId": "361c601b-7198-4c2e-a42e-c60f72518933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.7 (from pyspark==3.5.1)\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=bc1b876849c97bff10b032443a0176f293a27f5f53279089f38bc5f2791f65d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/91/5f/283b53010a8016a4ff1c4a1edd99bbe73afacb099645b5471b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.9\n",
            "    Uninstalling py4j-0.10.9.9:\n",
            "      Successfully uninstalled py4j-0.10.9.9\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 4.0.2\n",
            "    Uninstalling pyspark-4.0.2:\n",
            "      Successfully uninstalled pyspark-4.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 1.0.2 requires pyspark[connect]~=4.0.0, but you have pyspark 3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed py4j-0.10.9.7 pyspark-3.5.1\n",
            "Collecting pymongo\n",
            "  Downloading pymongo-4.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Collecting dnspython<3.0.0,>=2.6.1 (from pymongo)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading pymongo-4.16.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.8.0 pymongo-4.16.0\n",
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.12/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession object (entry point to use Spark)\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "    .appName(\"ECommerce-BigData-Pipeline\")\n",
        "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.1\")\n",
        "    .getOrCreate())"
      ],
      "metadata": {
        "id": "tzidA12p-nU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MongoDB config\n",
        "# 1. Setup\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# 2. Get Secrets\n",
        "try:\n",
        "    mongo_uri = userdata.get('MONGO_URI')\n",
        "    mongo_db = userdata.get('MONGO_DATABASE') # Fallback if secret missing\n",
        "\n",
        "    print(f\"Attempting to connect with User: {mongo_uri.split(':')[1].split('@')[0]}...\") # Prints username only for debug\n",
        "except Exception as e:\n",
        "    print(\"Error reading secrets! Make sure 'Notebook access' is ON in the sidebar.\")\n",
        "    raise e\n",
        "\n",
        "# 3. Connect & Test\n",
        "os.environ[\"MONGO_URI\"] = mongo_uri\n",
        "os.environ[\"MONGO_DATABASE\"] = mongo_db\n",
        "\n",
        "# 4. Verify connection\n",
        "try:\n",
        "    # Using the environment variable to connect, proving it works\n",
        "    client = MongoClient(os.environ[\"MONGO_URI\"])\n",
        "    client.admin.command('ping')\n",
        "\n",
        "    print(\"Authentication Successful! Connected to MongoDB Atlas.\")\n",
        "    print(f\"Environment variables set for database: {os.environ['MONGO_DATABASE']}\")\n",
        "    print(\"Ready for Spark integration.\")\n",
        "except Exception as e:\n",
        "    print(f\"Connection failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "tYBTOQCe2YdB",
        "outputId": "69e1b5b6-8a70-4442-e86b-e76d00b64aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error reading secrets! Make sure 'Notebook access' is ON in the sidebar.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret MONGO_URI does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3468121445.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error reading secrets! Make sure 'Notebook access' is ON in the sidebar.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 3. Connect & Test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3468121445.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 2. Get Secrets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmongo_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MONGO_URI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mmongo_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MONGO_DATABASE'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Fallback if secret missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret MONGO_URI does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Load Dataset"
      ],
      "metadata": {
        "id": "Ccc9LUCj_jEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8DZhrFOkDYD_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34217a56-34c6-4a0b-8c81-f8a44e11d131"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession class from PySpark SQL module\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Read the CSV file into a Spark DataFrame\n",
        "raw_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/drive/MyDrive/Bigdata Assignment 2 /Online retail.csv\")"
      ],
      "metadata": {
        "id": "Vd43EHE__nsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KgBAZ2vLYid6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Display schema and record counts"
      ],
      "metadata": {
        "id": "SECTXWGVGmtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df.printSchema()\n",
        "raw_df.count()"
      ],
      "metadata": {
        "id": "Ulk-hf0kGqeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f6dc7d-86cf-460b-ffa5-a1c5ba8db679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Invoice: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Customer ID: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "541910"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Store Bronze data as partitioned Parquet"
      ],
      "metadata": {
        "id": "WezmIFQwHNPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month, col, to_timestamp\n",
        "\n",
        "bronze_df = raw_df.withColumn(\n",
        "    \"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"M/d/yyyy H:mm\")\n",
        ").withColumn(\n",
        "    \"year\", year(\"InvoiceDate\")\n",
        ").withColumn(\n",
        "    \"month\", month(\"InvoiceDate\")\n",
        ")"
      ],
      "metadata": {
        "id": "13g6ij8uHSHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"year\", \"month\") \\\n",
        "    .parquet(\"/content/bronze\")"
      ],
      "metadata": {
        "id": "1NEi9xu1Hb8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Data Cleaning & Quality Management"
      ],
      "metadata": {
        "id": "1OFrUcR3ID7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Missing CustomerID Handling"
      ],
      "metadata": {
        "id": "XD9J3jGIJKH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, when\n",
        "\n",
        "null_customer = bronze_df.filter(col(\"Customer ID\").isNull()).count()\n",
        "print(f\"Null CustomerID records: {null_customer:,}\")"
      ],
      "metadata": {
        "id": "cL_KeFbjJOnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103b8c5e-b857-4545-cfe9-b9dde9c93b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null CustomerID records: 135,080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove null\n",
        "df1 = bronze_df.filter(col(\"Customer ID\").isNotNull())\n",
        "print(f\"After removing null CustomerID: {df1.count():,}\")"
      ],
      "metadata": {
        "id": "7p-rlqMeZowp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91ef901-9b18-456b-c9fc-bd0bdae8b6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After removing null CustomerID: 406,830\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Negative quantities (returns)"
      ],
      "metadata": {
        "id": "ysnE7OLJJt3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_qty_count = df1.filter(col(\"Quantity\") < 0).count()\n",
        "print(f\"Negative Quantity records: {negative_qty_count:,}\")\n",
        "df2 = df1.filter(col(\"Quantity\") > 0)\n",
        "print(f\"After removing returns: {df2.count():,}\")"
      ],
      "metadata": {
        "id": "j--ZGiTuKJdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3ff075-7ac5-4053-a03f-9427e82b35a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative Quantity records: 8,905\n",
            "After removing returns: 397,925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Cancelled invoices"
      ],
      "metadata": {
        "id": "Wg5cTaRvKwjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancelled_df = df2.filter(col(\"Invoice\").startswith(\"C\")).count()\n",
        "print(f\"Cancelled invoices (C prefix): {cancelled_df:,}\")\n",
        "df3 = df2.filter(~col(\"Invoice\").startswith(\"C\"))\n",
        "print(f\"After removing cancellations: {df3.count():,}\")"
      ],
      "metadata": {
        "id": "tgeo0w2By-CU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a15c429-3a0b-4503-c11a-33b5c8cef143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cancelled invoices (C prefix): 0\n",
            "After removing cancellations: 397,925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Invalid or extreme prices"
      ],
      "metadata": {
        "id": "8hVFO9aqLHKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "invalid_price_df = df3.filter(col(\"Price\") <= 0)\n",
        "invalid_price_count = invalid_price_df.count()\n",
        "print(f\"Invalid prices (<=0): {invalid_price_count:,}\")\n",
        "df4 = df3.filter(col(\"Price\") > 0)\n",
        "print(f\"After price filter: {df4.count():,}\")"
      ],
      "metadata": {
        "id": "HyLNB_ewLaq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f209f02-7122-405d-d26c-ec00079736f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid prices (<=0): 40\n",
            "After price filter: 397,885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Duplicate records"
      ],
      "metadata": {
        "id": "UdPiwETILHuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = df4.count()\n",
        "silver_df = df4.dropDuplicates()\n",
        "after = silver_df.count()\n",
        "duplicates_removed = before - after\n",
        "print(f\"Duplicates removed: {duplicates_removed:,}\")\n",
        "print(f\"Final Silver records: {after:,}\")"
      ],
      "metadata": {
        "id": "VuUxhMJgL7Sh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39b3093-69bf-4ca5-bbcb-ae5cc378a211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates removed: 5,192\n",
            "Final Silver records: 392,693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 Data Quality Report"
      ],
      "metadata": {
        "id": "ABNWIDZELINq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dq_report_df = spark.createDataFrame(\n",
        "    [{ \"Metric\": \"Total Raw Records\", \"Count\": raw_df.count() },\n",
        "    { \"Metric\": \"Null CustomerID Removed\", \"Count\": null_customer },\n",
        "    { \"Metric\": \"Negative Quantity Removed\", \"Count\": negative_qty_count },\n",
        "    { \"Metric\": \"Cancelled Invoices Removed\", \"Count\": cancelled_df },\n",
        "    { \"Metric\": \"Invalid Prices Removed\", \"Count\": invalid_price_count },\n",
        "    { \"Metric\": \"Duplicates Removed\", \"Count\": duplicates_removed },\n",
        "    { \"Metric\": \"FINAL SILVER Records\", \"Count\": after }],\n",
        "    [\"Metric\", \"Count\"])\n",
        "\n",
        "print(\"\\n DATA QUALITY REPORT\")\n",
        "dq_report_df.show(truncate=False)"
      ],
      "metadata": {
        "id": "2kr15JSoNMdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1d478f-9dc8-4650-ba44-3ff19881fb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " DATA QUALITY REPORT\n",
            "+------+--------------------------+\n",
            "|Metric|Count                     |\n",
            "+------+--------------------------+\n",
            "|541910|Total Raw Records         |\n",
            "|135080|Null CustomerID Removed   |\n",
            "|8905  |Negative Quantity Removed |\n",
            "|0     |Cancelled Invoices Removed|\n",
            "|40    |Invalid Prices Removed    |\n",
            "|5192  |Duplicates Removed        |\n",
            "|392693|FINAL SILVER Records      |\n",
            "+------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to save CSV\n",
        "folder_path = '/content/drive/MyDrive/Bigdata Assignment 2'\n",
        "output_path = os.path.join(folder_path, 'data_quality_report.csv')\n",
        "\n",
        "# Create the folder if it does not exist\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "# Export the Data Quality report\n",
        "try:\n",
        "    dq_report_df.toPandas().to_csv(output_path, index=False)\n",
        "    print(f\"\\nData Quality Report saved as '{output_path}'\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nCould not save to Drive. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "8vWz_wBuafE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c75e54-9470-4943-e23a-10020f2ad44a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data Quality Report saved as '/content/drive/MyDrive/Bigdata Assignment 2/data_quality_report.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Feature Engineering"
      ],
      "metadata": {
        "id": "Hh0uGk1jXxuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Load Silver Layer"
      ],
      "metadata": {
        "id": "-xj1102pXye2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the silver dataframe to the path so it can be loaded later\n",
        "silver_df.write.mode(\"overwrite\").parquet(\"/content/silver/online_retail\")\n",
        "\n",
        "# Read the silver layer from the parquet file\n",
        "df_silver = spark.read.parquet(\"/content/silver/online_retail\")\n",
        "df_silver.printSchema()"
      ],
      "metadata": {
        "id": "s_sxUK2zYF-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Revenue calculation"
      ],
      "metadata": {
        "id": "ROW_9bkDXzIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the col function to reference DataFrame columns\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create a new column 'revenue' by multiplying Quantity and UnitPrice\n",
        "df_feat = df_silver.withColumn(\n",
        "    \"revenue\",\n",
        "    col(\"Quantity\") * col(\"Price\")\n",
        ")"
      ],
      "metadata": {
        "id": "p8xdxKRBcjt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Time-based features (hour, weekday, month)"
      ],
      "metadata": {
        "id": "lurLFRhzXztd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import time-related functions from PySpark\n",
        "from pyspark.sql.functions import hour, dayofweek, month\n",
        "\n",
        "# Extract hour, day of week, and month from the InvoiceDate column\n",
        "df_feat = df_feat.withColumn(\"invoice_hour\", hour(\"InvoiceDate\")) \\\n",
        "                 .withColumn(\"weekday\", dayofweek(\"InvoiceDate\")) \\\n",
        "                 .withColumn(\"invoice_month\", month(\"InvoiceDate\"))"
      ],
      "metadata": {
        "id": "8BGbvM1Bcthj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Basket-level metrics"
      ],
      "metadata": {
        "id": "RCvokheeX0P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import aggregation functions from PySpark\n",
        "from pyspark.sql.functions import sum, countDistinct\n",
        "\n",
        "# Group by Invoice to calculate total revenue and number of unique items per basket\n",
        "basket_df = df_feat.groupBy(\"Invoice\").agg(\n",
        "    sum(\"revenue\").alias(\"basket_revenue\"),\n",
        "    countDistinct(\"StockCode\").alias(\"items_per_basket\")\n",
        ")\n",
        "\n",
        "# Join the basket metrics back to the main features dataframe\n",
        "df_feat = df_feat.join(basket_df, on=\"Invoice\", how=\"left\")"
      ],
      "metadata": {
        "id": "hyRGDcBXc3jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Customer RFM features"
      ],
      "metadata": {
        "id": "cccEFlpMX0zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import functions for calculating dates and maximum values\n",
        "from pyspark.sql.functions import max, datediff, current_date, countDistinct, sum\n",
        "\n",
        "# Group by Customer ID to calculate RFM (Recency, Frequency, Monetary) metrics:\n",
        "# Recency: Days between today and the last purchase\n",
        "# Frequency: Count of unique invoice numbers\n",
        "# Monetary: Sum of total revenue\n",
        "rfm_df = df_feat.groupBy(\"Customer ID\").agg(\n",
        "    datediff(current_date(), max(\"InvoiceDate\")).alias(\"recency\"),\n",
        "    countDistinct(\"Invoice\").alias(\"frequency\"),\n",
        "    sum(\"revenue\").alias(\"monetary\")\n",
        ")\n",
        "\n",
        "# Join the customer-level RFM features back to the main dataframe\n",
        "df_feat = df_feat.join(rfm_df, on=\"Customer ID\", how=\"left\")"
      ],
      "metadata": {
        "id": "lw_7WQXZc_U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Window-Based Feature"
      ],
      "metadata": {
        "id": "o2WCSf8fX1TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Window functionality and the sum aggregation function\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "# Define a window partitioned by Customer ID and ordered by InvoiceDate\n",
        "# It covers all rows from the start of the customer's history up to the current row\n",
        "customer_window = Window.partitionBy(\"Customer ID\") \\\n",
        "                        .orderBy(\"InvoiceDate\") \\\n",
        "                        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Calculate a running total of revenue for each customer using the defined window\n",
        "df_feat = df_feat.withColumn(\n",
        "    \"running_customer_spend\",\n",
        "    sum(\"revenue\").over(customer_window)\n",
        ")"
      ],
      "metadata": {
        "id": "0rSV5BR9dI_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Save Feature-Engineered Data"
      ],
      "metadata": {
        "id": "74NcRNOTX1yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the feature-engineered DataFrame to a Parquet file for persistence\n",
        "df_feat.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"/content/feature_engineered/online_retail\")"
      ],
      "metadata": {
        "id": "zErans3ydPs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: MongoDB Data Modeling"
      ],
      "metadata": {
        "id": "ybt3odJXP9b6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Start Spark with MongoDB Connector"
      ],
      "metadata": {
        "id": "ULRZvqu_i2CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# Create Spark session and tell Spark how to connect to MongoDB\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigData_Task4_MongoDB_GoldLayer\") \\\n",
        "    .config(\n",
        "        \"spark.mongodb.write.connection.uri\",\n",
        "        os.environ[\"MONGO_URI\"]\n",
        "    ) \\\n",
        "    .config(\n",
        "        \"spark.mongodb.write.database\",\n",
        "        os.environ[\"MONGO_DATABASE\"]\n",
        "    ) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session started successfully with MongoDB connector.\")"
      ],
      "metadata": {
        "id": "Kr5Vz27Ci-EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Load Feature-Engineered Data"
      ],
      "metadata": {
        "id": "F9nfTULjQGcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the feature-engineered dataset created in Task 3\n",
        "df_feat = spark.read.parquet(\"/content/feature_engineered/online_retail\")"
      ],
      "metadata": {
        "id": "8eW3Gx1mRKNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 COLLECTION 1 - fact_invoices"
      ],
      "metadata": {
        "id": "K1hP0A3GQKHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create invoice-level structure"
      ],
      "metadata": {
        "id": "2dLU3NlNWagM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import first, sum, collect_list, struct\n",
        "\n",
        "# Group by Invoice to create invoice-level documents\n",
        "fact_invoices_df = df_feat.groupBy(\"Invoice\").agg(\n",
        "\n",
        "    # Customer who placed the invoice\n",
        "    first(\"Customer ID\").alias(\"CustomerID\"),\n",
        "\n",
        "    # Invoice date\n",
        "    first(\"InvoiceDate\").alias(\"InvoiceDate\"),\n",
        "\n",
        "    # Country of purchase\n",
        "    first(\"Country\").alias(\"Country\"),\n",
        "\n",
        "    # Total revenue of the invoice\n",
        "    sum(\"revenue\").alias(\"total_invoice_revenue\"),\n",
        "\n",
        "    # Embed purchased items as an array\n",
        "    collect_list(\n",
        "        struct(\n",
        "            \"StockCode\",\n",
        "            \"Description\",\n",
        "            \"Quantity\",\n",
        "            \"Price\",\n",
        "            \"revenue\"\n",
        "        )\n",
        "    ).alias(\"line_items\")\n",
        ")"
      ],
      "metadata": {
        "id": "LXv0U2HsWgSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Schema Definition (fact_invoices)***\n",
        "\n",
        "\n",
        "{\n",
        "\n",
        "   _id: ObjectId,\n",
        "\n",
        "   Invoice: String,\n",
        "\n",
        "   CustomerID: Integer,\n",
        "\n",
        "   InvoiceDate: Date,\n",
        "\n",
        "   Country: String,\n",
        "\n",
        "   total_invoice_revenue: Double,\n",
        "\n",
        "   line_items: [\n",
        "     \n",
        "      {\n",
        "        StockCode: String,\n",
        "        Description: String,\n",
        "        Quantity: Integer,\n",
        "        Price: Double,\n",
        "        revenue: Double\n",
        "      }\n",
        "     ]\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "dKR8OdAlsTOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 COLLECTION 2 - dim_customers"
      ],
      "metadata": {
        "id": "1ruvskw9QTXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create customer summary"
      ],
      "metadata": {
        "id": "8_vpaxidX45B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import countDistinct, col, when\n",
        "\n",
        "# Create customer-level aggregation\n",
        "dim_customers_df = df_feat.groupBy(\"Customer ID\").agg(\n",
        "\n",
        "    # Recency (days since last purchase)\n",
        "    first(\"recency\").alias(\"recency\"),\n",
        "\n",
        "    # Frequency (number of invoices)\n",
        "    first(\"frequency\").alias(\"frequency\"),\n",
        "\n",
        "    # Monetary (total spend)\n",
        "    first(\"monetary\").alias(\"monetary\"),\n",
        "\n",
        "    # Total invoices count\n",
        "    countDistinct(\"Invoice\").alias(\"total_invoices\"),\n",
        "\n",
        "    # Total revenue from customer\n",
        "    sum(\"revenue\").alias(\"total_revenue\")\n",
        ")"
      ],
      "metadata": {
        "id": "R56oVl49Xlaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add customer segmentation"
      ],
      "metadata": {
        "id": "KT7nDmN7YCJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Segment customers based on monetary value\n",
        "dim_customers_df = dim_customers_df.withColumn(\n",
        "    \"customer_segment\",\n",
        "    when(col(\"monetary\") >= 5000, \"High Value\")\n",
        "    .when(col(\"monetary\") >= 2000, \"Medium Value\")\n",
        "    .otherwise(\"Low Value\")\n",
        ")"
      ],
      "metadata": {
        "id": "9m21ZgR3YNZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Schema Definition (dim_customers)***\n",
        "\n",
        "{\n",
        "\n",
        "  _id: ObjectId,\n",
        "\n",
        "  CustomerID: Integer,\n",
        "\n",
        "  recency: Integer,\n",
        "\n",
        "  frequency: Integer,\n",
        "\n",
        "  monetary: Double,\n",
        "\n",
        "  total_invoices: Integer,\n",
        "\n",
        "  total_revenue: Double,\n",
        "\n",
        "  customer_segment: String\n",
        "  \n",
        "}"
      ],
      "metadata": {
        "id": "V2oYoIgiuXC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 COLLECTION 3 - dim_products"
      ],
      "metadata": {
        "id": "UI3-lfvuQVog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create product summary"
      ],
      "metadata": {
        "id": "gcvXMwTHZldv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import collect_set\n",
        "\n",
        "# Create product-level performance data\n",
        "dim_products_df = df_feat.groupBy(\"StockCode\", \"Description\").agg(\n",
        "\n",
        "    # Total quantity sold\n",
        "    sum(\"Quantity\").alias(\"total_quantity_sold\"),\n",
        "\n",
        "    # Total revenue of product\n",
        "    sum(\"revenue\").alias(\"total_product_revenue\"),\n",
        "\n",
        "    # Countries where product was sold\n",
        "    collect_set(\"Country\").alias(\"countries_sold\")\n",
        ")"
      ],
      "metadata": {
        "id": "H-kJKfe0Y3VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Schema Definition (dim_products)***\n",
        "\n",
        "{\n",
        "\n",
        "  _id: ObjectId,\n",
        "\n",
        "  StockCode: String,\n",
        "\n",
        "  Description: String,\n",
        "\n",
        "  total_quantity_sold: Integer,\n",
        "\n",
        "  total_product_revenue: Double,\n",
        "\n",
        "  countries_sold: [String]\n",
        "  \n",
        "}"
      ],
      "metadata": {
        "id": "WfKSbmc6u8Uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5 MongoDB Indexing & Write Optimization"
      ],
      "metadata": {
        "id": "NYZ1rT6hBVIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 Write Gold datasets to MongoDB"
      ],
      "metadata": {
        "id": "spPTYWZYCw1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write to MongoDB (V10 Syntax)\n",
        "fact_invoices_df.write \\\n",
        "    .format(\"mongodb\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"spark.mongodb.connection.uri\", os.environ.get(\"MONGO_URI\")) \\\n",
        "    .option(\"spark.mongodb.database\", os.environ.get(\"MONGO_DATABASE\")) \\\n",
        "    .option(\"spark.mongodb.collection\", \"fact_invoices\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"Data successfully written to MongoDB!\")"
      ],
      "metadata": {
        "id": "wGIRcS7guT_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write dim_customers to MongoDB (V10 Syntax)\n",
        "dim_customers_df.write \\\n",
        "    .format(\"mongodb\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"spark.mongodb.connection.uri\", os.environ.get(\"MONGO_URI\")) \\\n",
        "    .option(\"spark.mongodb.database\", os.environ.get(\"MONGO_DATABASE\")) \\\n",
        "    .option(\"spark.mongodb.collection\", \"dim_customers\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"dim_customers data successfully written to MongoDB!\")"
      ],
      "metadata": {
        "id": "wGg20d2kzCq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write dim_products to MongoDB (V10 Syntax)\n",
        "dim_products_df.write \\\n",
        "    .format(\"mongodb\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"spark.mongodb.connection.uri\", os.environ.get(\"MONGO_URI\")) \\\n",
        "    .option(\"spark.mongodb.database\", os.environ.get(\"MONGO_DATABASE\")) \\\n",
        "    .option(\"spark.mongodb.collection\", \"dim_products\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"dim_products data successfully written to MongoDB!\")"
      ],
      "metadata": {
        "id": "G_JItyCGzSAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 Create minimum 4 indexes"
      ],
      "metadata": {
        "id": "ebKXRPPNCE9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyMongo tools for indexing\n",
        "from pymongo import MongoClient, ASCENDING, DESCENDING\n",
        "import os\n",
        "\n",
        "# Connect to MongoDB using existing environment variables\n",
        "client = MongoClient(os.environ[\"MONGO_URI\"])\n",
        "db = client[os.environ[\"MONGO_DATABASE\"]]\n",
        "\n",
        "print(\"Creating Indexes...\")"
      ],
      "metadata": {
        "id": "JHKhKdwuCIpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Index on CustomerID (Ascending)\n",
        "db.fact_invoices.create_index([(\"CustomerID\", ASCENDING)])\n",
        "print(\"Index 1 Created: fact_invoices.CustomerID (ASC)\")"
      ],
      "metadata": {
        "id": "KstuTD_YDffv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Index on InvoiceDate (Descending)\n",
        "db.fact_invoices.create_index([(\"InvoiceDate\", DESCENDING)])\n",
        "print(\"Index 2 Created: fact_invoices.InvoiceDate (DESC)\")"
      ],
      "metadata": {
        "id": "NOWo3YmSDiQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Index on customer_segment (Ascending)\n",
        "db.dim_customers.create_index([(\"customer_segment\", ASCENDING)])\n",
        "print(\"Index 3 Created: dim_customers.customer_segment (ASC)\")"
      ],
      "metadata": {
        "id": "Tbo6KW_-DkJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Index on total_product_revenue (Descending)\n",
        "db.dim_products.create_index([(\"total_product_revenue\", DESCENDING)])\n",
        "print(\"Index 4 Created: dim_products.total_product_revenue (DESC)\")"
      ],
      "metadata": {
        "id": "PQ4HT5BzDm-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3 Justify each index"
      ],
      "metadata": {
        "id": "0i8Ve9ZmJEr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On report"
      ],
      "metadata": {
        "id": "0QDvhGjsJVNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.4 Demonstrate query performance improvement"
      ],
      "metadata": {
        "id": "z-6xTUIgJObZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print(\"Testing Performance on CustomerID Query...\")\n",
        "\n",
        "# sample query (finding invoices for customer 17850)\n",
        "sample_query = {\"CustomerID\": 17850}\n",
        "\n",
        "#MongoDB explain how it will execute this query\n",
        "explanation = db.fact_invoices.find(sample_query).explain()\n",
        "\n",
        "# Extract the winning execution plan strategy\n",
        "winning_stage = explanation['queryPlanner']['winningPlan']['stage']\n",
        "\n",
        "# Sometimes the IXSCAN is nested under a FETCH stage\n",
        "if winning_stage == 'FETCH':\n",
        "    winning_stage = explanation['queryPlanner']['winningPlan']['inputStage']['stage']\n",
        "\n",
        "print(f\"Execution Strategy Used: {winning_stage}\")\n",
        "\n",
        "if winning_stage == \"IXSCAN\":\n",
        "    print(\"SUCCESS! MongoDB utilized the Index (IXSCAN).\")\n",
        "    print(\"Without the index, this would have been a 'COLLSCAN', forcing MongoDB to read every single document.\")\n",
        "else:\n",
        "    print(\"Warning: Index was not used.\")"
      ],
      "metadata": {
        "id": "ngHbKS9YJLQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 6 (Analytics & Insights)"
      ],
      "metadata": {
        "id": "pP2xZD06K2vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Spark-based analytics queries"
      ],
      "metadata": {
        "id": "EGJmURD_K8xR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, desc, count, col\n",
        "\n",
        "# 1. Monthly Revenue Trends\n",
        "print(\"1. Monthly Revenue Trends\")\n",
        "monthly_revenue_spark = df_feat.groupBy(\"year\", \"month\") \\\n",
        "    .agg(sum(\"revenue\").alias(\"Total_Revenue\")) \\\n",
        "    .orderBy(\"year\", \"month\")\n",
        "monthly_revenue_spark.show()\n",
        "\n",
        "# 2. Top Customers by Spend\n",
        "print(\"2. Top 5 Customers by Spend\")\n",
        "top_customers_spark = dim_customers_df \\\n",
        "    .orderBy(desc(\"monetary\")) \\\n",
        "    .select(\"Customer ID\", \"monetary\", \"total_invoices\", \"customer_segment\")\n",
        "top_customers_spark.show(5)\n",
        "\n",
        "# 3. Top Products by Revenue\n",
        "print(\"3. Top 5 Products by Revenue\")\n",
        "top_products_spark = dim_products_df \\\n",
        "    .orderBy(desc(\"total_product_revenue\")) \\\n",
        "    .select(\"StockCode\", \"Description\", \"total_product_revenue\", \"total_quantity_sold\")\n",
        "top_products_spark.show(5)\n",
        "\n",
        "# 4. Country-Level Sales Analysis\n",
        "print(\"4. Top 5 Countries by Sales Revenue\")\n",
        "country_sales_spark = df_feat.groupBy(\"Country\") \\\n",
        "    .agg(sum(\"revenue\").alias(\"Total_Revenue\"), count(\"Invoice\").alias(\"Total_Transactions\")) \\\n",
        "    .orderBy(desc(\"Total_Revenue\"))\n",
        "country_sales_spark.show(5)\n",
        "\n",
        "# 5. Return or Cancellation Patterns (Using Bronze Data)\n",
        "print(\"5. Most Frequently Returned/Cancelled Products\")\n",
        "# We use bronze_df here because Silver/Gold layers filtered out returns!\n",
        "returns_spark = bronze_df.filter(col(\"Quantity\") < 0) \\\n",
        "    .groupBy(\"Description\") \\\n",
        "    .agg(sum(\"Quantity\").alias(\"Total_Returned_Qty\"), count(\"Invoice\").alias(\"Return_Count\")) \\\n",
        "    .orderBy(\"Total_Returned_Qty\") # Ordering ascending because quantities are negative\n",
        "returns_spark.show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "csuWoJk9K6kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 MongoDB Aggregation Pipelines"
      ],
      "metadata": {
        "id": "zAfgzRwdLaal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Monthly Revenue Trends (Using fact_invoices)\n",
        "print(\"1. Monthly Revenue Trends (MongoDB)\")\n",
        "pipeline_monthly = [\n",
        "    {\"$group\": {\n",
        "        \"_id\": {\"year\": {\"$year\": \"$InvoiceDate\"}, \"month\": {\"$month\": \"$InvoiceDate\"}},\n",
        "        \"total_revenue\": {\"$sum\": \"$total_invoice_revenue\"}\n",
        "    }},\n",
        "    {\"$sort\": {\"_id.year\": 1, \"_id.month\": 1}}\n",
        "]\n",
        "res_monthly = list(db.fact_invoices.aggregate(pipeline_monthly))\n",
        "# Using pandas just for clean printing in Colab\n",
        "print(pd.json_normalize(res_monthly).to_string(index=False), \"\\n\")\n",
        "\n",
        "\n",
        "# 2. Top Customers by Spend (Using dim_customers)\n",
        "print(\"2. Top 5 Customers by Spend (MongoDB)\")\n",
        "pipeline_top_cust = [\n",
        "    {\"$sort\": {\"monetary\": -1}},\n",
        "    {\"$limit\": 5},\n",
        "    {\"$project\": {\"Customer ID\": 1, \"monetary\": 1, \"customer_segment\": 1, \"_id\": 0}}\n",
        "]\n",
        "res_top_cust = list(db.dim_customers.aggregate(pipeline_top_cust))\n",
        "print(pd.DataFrame(res_top_cust).to_string(index=False), \"\\n\")\n",
        "\n",
        "\n",
        "# 3. Top Products by Revenue (Using dim_products)\n",
        "print(\"3. Top 5 Products by Revenue (MongoDB)\")\n",
        "pipeline_top_prod = [\n",
        "    {\"$sort\": {\"total_product_revenue\": -1}},\n",
        "    {\"$limit\": 5},\n",
        "    {\"$project\": {\"Description\": 1, \"total_product_revenue\": 1, \"total_quantity_sold\": 1, \"_id\": 0}}\n",
        "]\n",
        "res_top_prod = list(db.dim_products.aggregate(pipeline_top_prod))\n",
        "print(pd.DataFrame(res_top_prod).to_string(index=False), \"\\n\")\n",
        "\n",
        "\n",
        "# 4. Country-Level Sales Analysis (Using fact_invoices)\n",
        "print(\"4. Top 5 Countries by Sales (MongoDB)\")\n",
        "pipeline_country = [\n",
        "    {\"$group\": {\n",
        "        \"_id\": \"$Country\",\n",
        "        \"total_sales\": {\"$sum\": \"$total_invoice_revenue\"},\n",
        "        \"invoice_count\": {\"$sum\": 1}\n",
        "    }},\n",
        "    {\"$sort\": {\"total_sales\": -1}},\n",
        "    {\"$limit\": 5}\n",
        "]\n",
        "res_country = list(db.fact_invoices.aggregate(pipeline_country))\n",
        "print(pd.DataFrame(res_country).to_string(index=False), \"\\n\")\n",
        "\n",
        "\n",
        "# 5. Customer Segmentation Distribution (Using dim_customers)\n",
        "print(\"5. Customer Segmentation Value (MongoDB)\")\n",
        "pipeline_segments = [\n",
        "    {\"$group\": {\n",
        "        \"_id\": \"$customer_segment\",\n",
        "        \"customer_count\": {\"$sum\": 1},\n",
        "        \"total_segment_revenue\": {\"$sum\": \"$monetary\"}\n",
        "    }},\n",
        "    {\"$sort\": {\"total_segment_revenue\": -1}}\n",
        "]\n",
        "res_segments = list(db.dim_customers.aggregate(pipeline_segments))\n",
        "print(pd.DataFrame(res_segments).to_string(index=False), \"\\n\")"
      ],
      "metadata": {
        "id": "MebRnML6LSKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7 Performance Optimization"
      ],
      "metadata": {
        "id": "G4GEClJ1MWDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 Partitioning strategies"
      ],
      "metadata": {
        "id": "lo0As-GiYZWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data is partitioned by year and month to improve query performance.\n",
        "# Spark reads only required partitions during analysis.\n",
        "bronze_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"year\", \"month\") \\\n",
        "    .parquet(\"/content/bronze\")"
      ],
      "metadata": {
        "id": "JZE57J_sXsU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.2 Caching and persistence"
      ],
      "metadata": {
        "id": "_AvF60sRaMRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import StorageLevel\n",
        "\n",
        "# Persist dataframe in memory and disk(storage strategy Spark uses when we persist/cache a DataFrame )\n",
        "# MEMORY_AND_DISK avoids recomputation and prevents memory overflow errors\n",
        "df_feat.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "# Force Spark to materialize cache immediately\n",
        "# Otherwise caching is lazy and may not activate when expected\n",
        "df_feat.count()\n"
      ],
      "metadata": {
        "id": "xAP53G6oacL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.3 Broadcast Join (Shuffle Reduction)"
      ],
      "metadata": {
        "id": "D6KKV4TgnbVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Broadcast join optimization\n",
        "# basket_df is smaller than df_feat,broadcasting the smaller DataFrame allows Spark to replicate it across executors rather than shuffling both datasets avoids expensive shuffle operations\n",
        "#,which significantly improves join performance.\n",
        "\n",
        "df_feat = df_feat.join(basket_df, on=\"Invoice\", how=\"left\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OWBUt6EDnrHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}